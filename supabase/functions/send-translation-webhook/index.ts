import { createClient } from "npm:@supabase/supabase-js@2";

const corsHeaders = {
  "Access-Control-Allow-Origin": "*",
  "Access-Control-Allow-Methods": "POST, OPTIONS",
  "Access-Control-Allow-Headers": "Content-Type, Authorization",
};

// Cache para evitar processamento duplicado
const processedRequests = new Map<string, number>();

Deno.serve(async (req: Request) => {
  console.log(`[${new Date().toISOString()}] Edge Function: send-translation-webhook called`);
  console.log(`Method: ${req.method}`);
  console.log(`URL: ${req.url}`);

  // Handle CORS preflight
  if (req.method === "OPTIONS") {
    console.log("Handling CORS preflight request");
    return new Response(null, {
      status: 200,
      headers: corsHeaders,
    });
  }

  if (req.method !== "POST") {
    console.log(`Method ${req.method} not allowed`);
    return new Response("Method Not Allowed", {
      status: 405,
      headers: corsHeaders,
    });
  }

  try {
    // Create Supabase client with service role key
    const supabaseUrl = Deno.env.get("SUPABASE_URL");
    const supabaseServiceKey = Deno.env.get("SUPABASE_SERVICE_ROLE_KEY");
    
    console.log("Supabase URL:", supabaseUrl ? "‚úì Set" : "‚úó Missing");
    console.log("Service Role Key:", supabaseServiceKey ? "‚úì Set" : "‚úó Missing");

    if (!supabaseUrl || !supabaseServiceKey) {
      throw new Error("Missing Supabase environment variables");
    }

    const supabase = createClient(supabaseUrl, supabaseServiceKey);

    // Parse request body
    const requestBody = await req.text();
    console.log("=== WEBHOOK CALL START ===");
    console.log("Timestamp:", new Date().toISOString());
    console.log("Raw request body:", requestBody);
    
    const parsedBody = JSON.parse(requestBody);
    console.log("Parsed request body:", parsedBody);
    console.log("Request headers:", Object.fromEntries(req.headers.entries()));

    // Log adicional para identificar a fonte da chamada
    const referer = req.headers.get('referer') || 'unknown';
    const origin = req.headers.get('origin') || 'unknown';
    console.log("=== REQUEST SOURCE INFO ===");
    console.log("Referer:", referer);
    console.log("Origin:", origin);
    console.log("User-Agent:", req.headers.get('user-agent') || 'unknown');

    // Gerar um ID √∫nico para esta requisi√ß√£o baseado no conte√∫do (SEM timestamp para detectar duplicatas reais)
    const requestId = `${parsedBody.user_id || 'unknown'}_${parsedBody.filename || 'unknown'}`;
    console.log("Request ID:", requestId);
    
    // VERIFICA√á√ÉO ROBUSTA DE DUPLICATAS USANDO BANCO DE DADOS
    // Cache em mem√≥ria pode falhar se houver m√∫ltiplas inst√¢ncias da Edge Function
    if (parsedBody.user_id && parsedBody.filename) {
      console.log("üîç VERIFICA√á√ÉO ANTI-DUPLICATA: Checando banco de dados...");
      
      const cutoffTime = new Date(Date.now() - 2 * 60 * 1000).toISOString(); // 2 minutos atr√°s
      
      const { data: recentDocs, error: recentError } = await supabase
        .from('documents_to_be_verified')
        .select('id, filename, created_at')
        .eq('user_id', parsedBody.user_id)
        .eq('filename', parsedBody.filename)
        .gte('created_at', cutoffTime)
        .order('created_at', { ascending: false })
        .limit(1);

      if (recentError) {
        console.log("‚ö†Ô∏è Erro ao verificar duplicatas:", recentError);
      } else if (recentDocs && recentDocs.length > 0) {
        console.log("üö® DUPLICATA DETECTADA! Documento j√° processado recentemente:");
        console.log("Documento existente:", recentDocs[0]);
        console.log("‚è±Ô∏è Criado em:", recentDocs[0].created_at);
        console.log("‚úÖ IGNORANDO upload duplicado para prevenir m√∫ltiplos documentos");
        
        return new Response(
          JSON.stringify({
            success: true,
            status: 200,
            message: "Document already processed recently - duplicate prevented",
            existing_document: recentDocs[0],
            timestamp: new Date().toISOString()
          }),
          {
            status: 200,
            headers: {
              "Content-Type": "application/json",
              ...corsHeaders,
            },
          }
        );
      } else {
        console.log("‚úÖ Nenhuma duplicata encontrada, prosseguindo com o upload");
      }
    }
    
    // Cache em mem√≥ria como backup (pode n√£o funcionar com m√∫ltiplas inst√¢ncias)
    const now = Date.now();
    const lastProcessed = processedRequests.get(requestId);
    if (lastProcessed && (now - lastProcessed) < 120000) {
      console.log("üîÑ Cache em mem√≥ria detectou duplicata");
      return new Response(
        JSON.stringify({
          success: true,
          status: 200,
          message: "Request already processed (memory cache)",
          timestamp: new Date().toISOString()
        }),
        {
          status: 200,
          headers: {
            "Content-Type": "application/json",
            ...corsHeaders,
          },
        }
      );
    }
    
    // Marcar esta requisi√ß√£o como processada
    processedRequests.set(requestId, now);
    
    // Limpar cache antigo (mais de 5 minutos)
    for (const [key, timestamp] of processedRequests.entries()) {
      if (now - timestamp > 300000) { // 5 minutos
        processedRequests.delete(key);
      }
    }

    // Recebe o evento do Supabase Storage ou do frontend
    const { 
      filename, 
      url, 
      mimetype, 
      size, 
      record, 
      user_id, 
      pages,
      paginas,
      document_type,
      tipo_trad, 
      total_cost,
      valor, 
      source_language,
      target_language,
      idioma_raiz,
      idioma_destino,
      is_bank_statement, 
      client_name,
      source_currency,
      target_currency,
      document_id,
      original_document_id,
      original_filename
    } = parsedBody;
    
    // Debug logs para idiomas e moedas
    console.log("=== LANGUAGE & CURRENCY DEBUG ===");
    console.log("source_language:", source_language);
    console.log("target_language:", target_language);
    console.log("idioma_raiz:", idioma_raiz);
    console.log("idioma_destino:", idioma_destino);
    console.log("source_currency:", source_currency);
    console.log("target_currency:", target_currency);
    console.log("is_bank_statement:", is_bank_statement);
    
    // Buscar original_filename da tabela documents se n√£o estiver no payload
    let finalOriginalFilename = original_filename;
    if (!finalOriginalFilename && document_id) {
      console.log("üîç Buscando original_filename na tabela documents para document_id:", document_id);
      const { data: docData, error: docError } = await supabase
        .from('documents')
        .select('original_filename, filename')
        .eq('id', document_id)
        .single();
      
      if (docError) {
        console.error("‚ùå Erro ao buscar documento:", docError);
      } else {
        finalOriginalFilename = docData.original_filename || docData.filename;
        console.log("‚úÖ original_filename encontrado:", finalOriginalFilename);
      }
    }
    
    let payload;

    if (record) {
      // Called from Storage trigger
      console.log("Processing storage trigger payload");
      const bucket = record.bucket_id || record.bucket || record.bucketId || 'documents';
      const path = record.name || record.path || record.file_name;
      
      // IMPORTANTE: Como os buckets s√£o privados, gerar signed URL usando service_role
      let publicUrl;
      if (url && url.startsWith('http') && url.includes('sign')) {
        // Se j√° temos uma signed URL v√°lida, usar ela
        publicUrl = url;
        console.log("Using existing signed URL:", publicUrl);
      } else {
        // Gerar signed URL v√°lido por 24 horas (86400 segundos)
        // Usando service_role, funciona mesmo com buckets privados
        try {
          const { data: signedData, error: signedError } = await supabase.storage
            .from(bucket)
            .createSignedUrl(path, 86400); // 24 horas
          
          if (signedData?.signedUrl) {
            publicUrl = signedData.signedUrl;
            console.log("Generated signed URL from storage trigger (valid for 24h):", publicUrl);
          } else if (signedError) {
            console.error("Error generating signed URL from storage trigger:", signedError);
            // Fallback: tentar URL p√∫blica (pode n√£o funcionar se bucket for privado)
            const { data: { publicUrl: fallbackUrl } } = supabase.storage
              .from(bucket)
              .getPublicUrl(path);
            publicUrl = fallbackUrl;
            console.log("Fallback to public URL (may not work if bucket is private):", publicUrl);
          }
        } catch (urlError) {
          console.error("Error generating URL from storage trigger:", urlError);
          // √öltimo fallback: tentar URL p√∫blica
          try {
            const { data: { publicUrl: fallbackUrl } } = supabase.storage
              .from(bucket)
              .getPublicUrl(path);
            publicUrl = fallbackUrl;
            console.log("Final fallback to public URL:", publicUrl);
          } catch (fallbackError) {
            console.error("All URL generation methods failed:", fallbackError);
            publicUrl = url || `https://${supabaseUrl}/storage/v1/object/${bucket}/${path}`;
          }
        }
      }
      
      console.log("Final URL for n8n:", publicUrl);
      
      payload = {
        filename: path,
        url: publicUrl,
        mimetype: record.mimetype || record.metadata?.mimetype || "application/octet-stream",
        size: record.size || record.metadata?.size || null,
        user_id: record.user_id || record.metadata?.user_id || null,
        // Sempre usar campos padronizados
        pages: record.pages || pages || paginas || 1,
        document_type: 'Certificado', // Sempre usar 'Certificado' em portugu√™s
        total_cost: record.total_cost || total_cost || record.valor || valor || '0',
        source_language: record.source_language || source_language || record.idioma_raiz || idioma_raiz,
        target_language: record.target_language || target_language || record.idioma_destino || idioma_destino,
        is_bank_statement: record.is_bank_statement || is_bank_statement || false,
        client_name: record.client_name || client_name || null,
        // Campos de moeda para bank statements
        source_currency: record.source_currency || source_currency || null,
        target_currency: record.target_currency || target_currency || null,
        // Campos para identifica√ß√£o do documento original
        original_document_id: record.original_document_id || original_document_id || document_id || null,
        original_filename: finalOriginalFilename,
        // Adicionar informa√ß√µes sobre o tipo de arquivo
        isPdf: (record.mimetype || record.metadata?.mimetype || "application/octet-stream") === 'application/pdf',
        fileExtension: path.split('.').pop()?.toLowerCase(),
        // Informar ao n8n que deve usar a tabela 'profiles' em vez de 'users'
        tableName: 'profiles',
        schema: 'public'
      };
    } else {
      // Called from frontend
      console.log("Processing frontend payload");
      console.log("URL received:", url);
      console.log("User ID:", user_id);
      console.log("Filename:", filename);
      
      // Verificar se a URL j√° √© v√°lida
      let finalUrl = url;
      if (url && !url.startsWith('http')) {
        // Se a URL n√£o √© completa (√© apenas um filePath), gerar um signed URL
        // IMPORTANTE: Como os buckets s√£o privados, precisamos gerar signed URL usando service_role
        try {
          // Extrair o caminho do arquivo da URL
          const urlParts = url.split('/');
          const fileName = urlParts[urlParts.length - 1];
          const userFolder = urlParts[urlParts.length - 2];
          const filePath = `${userFolder}/${fileName}`;
          
          console.log("Extracted file path:", filePath);
          
          // Detectar bucket baseado no caminho
          let bucket = 'documents';
          if (filePath.includes('arquivosfinaislush')) {
            bucket = 'arquivosfinaislush';
          }
          
          // Gerar signed URL v√°lido por 24 horas (86400 segundos)
          // Usando service_role, funciona mesmo com buckets privados
          const { data: signedData, error: signedError } = await supabase.storage
            .from(bucket)
            .createSignedUrl(filePath, 86400); // 24 horas
          
          if (signedData?.signedUrl) {
            finalUrl = signedData.signedUrl;
            console.log("Generated signed URL from path (valid for 24h):", finalUrl);
          } else if (signedError) {
            console.error("Error generating signed URL:", signedError);
            // Tentar fallback: construir URL direta (pode n√£o funcionar se bucket for privado)
            const { data: { publicUrl: fallbackUrl } } = supabase.storage
              .from(bucket)
              .getPublicUrl(filePath);
            finalUrl = fallbackUrl;
            console.log("Fallback to public URL (may not work if bucket is private):", finalUrl);
          }
        } catch (urlError) {
          console.error("Error generating URL:", urlError);
          // Usar a URL original se n√£o conseguir gerar
          finalUrl = url;
        }
      } else if (url && url.startsWith('http') && url.includes('supabase.co')) {
        // Se j√° √© uma URL do Supabase, verificar se √© signed URL v√°lida
        // Se n√£o for signed URL e o bucket for privado, tentar gerar signed URL
        try {
          // Extrair filePath da URL existente
          const urlObj = new URL(url);
          const pathParts = urlObj.pathname.split('/').filter(p => p);
          
          // Detectar bucket
          let bucket = 'documents';
          if (pathParts.includes('arquivosfinaislush')) {
            bucket = 'arquivosfinaislush';
          }
          
          // Se n√£o √© signed URL (n√£o tem 'sign' no path), tentar gerar
          if (!pathParts.includes('sign')) {
            // Tentar extrair filePath da URL
            const bucketIndex = pathParts.findIndex(p => p === bucket);
            if (bucketIndex >= 0) {
              const filePath = pathParts.slice(bucketIndex + 1).join('/');
              console.log("Extracted filePath from existing URL:", filePath);
              
              // Gerar signed URL v√°lido por 24 horas
              const { data: signedData, error: signedError } = await supabase.storage
                .from(bucket)
                .createSignedUrl(filePath, 86400);
              
              if (signedData?.signedUrl) {
                finalUrl = signedData.signedUrl;
                console.log("Generated signed URL from existing URL (valid for 24h):", finalUrl);
              } else if (signedError) {
                console.error("Error generating signed URL from existing URL:", signedError);
              }
            }
          }
        } catch (urlError) {
          console.error("Error processing existing URL:", urlError);
          // Manter URL original
        }
      }
      
      payload = { 
        filename: filename, 
        url: finalUrl, 
        mimetype, 
        size, 
        user_id: user_id || null, 
        // Sempre usar campos padronizados
        pages: pages || paginas || 1,
        document_type: 'Certificado', // Sempre usar 'Certificado' em portugu√™s
        total_cost: total_cost || valor || '0',
        source_language: source_language || idioma_raiz,
        target_language: target_language || idioma_destino,
        is_bank_statement: is_bank_statement || false,
        client_name: client_name || null,
        // Campos de moeda para bank statements
        source_currency: source_currency || null,
        target_currency: target_currency || null,
        // Campos para identifica√ß√£o do documento original
        original_document_id: original_document_id || document_id || null,
        original_filename: finalOriginalFilename,
        // Adicionar informa√ß√µes sobre o tipo de arquivo
        isPdf: mimetype === 'application/pdf',
        fileExtension: filename.split('.').pop()?.toLowerCase(),
        // Informar ao n8n que deve usar a tabela 'profiles' em vez de 'users'
        tableName: 'profiles',
        schema: 'public'
      };
      
      console.log("Final payload for frontend:", JSON.stringify(payload, null, 2));
    }

    console.log("Final payload for n8n webhook:", JSON.stringify(payload, null, 2));

    // Verificar se o arquivo √© um PDF antes de enviar para o n8n
    const isPdf = payload.isPdf || payload.mimetype === 'application/pdf' || payload.filename.toLowerCase().endsWith('.pdf');
    console.log("Is PDF file:", isPdf);
    console.log("File mimetype:", payload.mimetype);
    console.log("File extension:", payload.fileExtension);
    console.log("Table name for n8n:", payload.tableName);

    // Send POST to n8n webhook
    const webhookUrl = "https://nwh.thefutureofenglish.com/webhook/thelushamericatranslations";
    console.log("Sending webhook to:", webhookUrl);
    console.log("Payload being sent to n8n:", JSON.stringify(payload, null, 2));

    const webhookResponse = await fetch(webhookUrl, {
      method: "POST",
      headers: { 
        "Content-Type": "application/json",
        "User-Agent": "Supabase-Edge-Function/1.0"
      },
      body: JSON.stringify(payload),
    });
    const responseText = await webhookResponse.text();
    console.log("n8n webhook response status:", webhookResponse.status);
    console.log("n8n webhook response headers:", Object.fromEntries(webhookResponse.headers.entries()));
    console.log("n8n webhook response body:", responseText);

    // If webhook call was successful and we have user_id, update document status
    if (webhookResponse.ok && user_id && filename) {
      try {
        console.log("Updating document status to processing...");
        console.log("Looking for document with user_id:", user_id, "and filename:", filename);
        
        const { data: updateData, error: updateError } = await supabase
          .from('documents')
          .update({ status: 'processing' })
          .eq('user_id', user_id)
          .eq('filename', filename)
          .select();
        
        if (updateError) {
          console.error("Error updating document status:", updateError);
        } else {
          console.log("Document status updated successfully:", updateData);
        }
      } catch (updateError) {
        console.error("Exception updating document status:", updateError);
      }
    }

    // üìã FLUXO CORRETO: Apenas enviar para n8n
    // O retorno do webhook do n8n √© que vai salvar na tabela documents_to_be_verified
    if (webhookResponse.ok) {
      console.log("‚úÖ Webhook enviado para n8n com sucesso");
      console.log("üìã O retorno do n8n ser√° respons√°vel por salvar em documents_to_be_verified");
      console.log("üö´ Edge Function N√ÉO deve inserir diretamente em documents_to_be_verified");
    }

    const responseData = {
      success: webhookResponse.ok,
      status: webhookResponse.status,
      message: responseText,
      payload: payload,
      timestamp: new Date().toISOString()
    };

    console.log("Final response:", JSON.stringify(responseData, null, 2));

    return new Response(
      JSON.stringify(responseData),
      {
        status: webhookResponse.ok ? 200 : 500,
        headers: {
          "Content-Type": "application/json",
          ...corsHeaders,
        },
      }
    );

  } catch (error) {
    console.error("Error in send-translation-webhook:", error);
    console.error("Error stack:", error.stack);
    
    const errorResponse = {
      success: false,
      error: error.message,
      stack: error.stack,
      timestamp: new Date().toISOString()
    };

    return new Response(
      JSON.stringify(errorResponse),
      {
        status: 500,
        headers: {
          "Content-Type": "application/json",
          ...corsHeaders,
        },
      }
    );
  }
});